# Orpheus-FastAPI Configuration
# Copy this file to .env and customize as needed

# Server connection settings
# For native installation with start.sh, use port 5006
# For LM Studio or other servers, adjust port accordingly (e.g., 1234)
ORPHEUS_API_URL=http://127.0.0.1:5006/v1/completions
ORPHEUS_API_TIMEOUT=120 # You should scale this value based on max tokens and your inference speed

# Generation parameters
ORPHEUS_MAX_TOKENS=8192 # If you want longer completions, increase this value
ORPHEUS_TEMPERATURE=0.6
ORPHEUS_TOP_P=0.9
# Repetition penalty is now hardcoded to 1.1 for stability (this is a model constraint) - this setting is no longer used
# ORPHEUS_REPETITION_PENALTY=1.1
ORPHEUS_SAMPLE_RATE=24000
ORPHEUS_MODEL_NAME=Orpheus-3b-FT-Q8_0.gguf # Model name sent to inference server (Q2_K, Q4_K_M, or Q8_0 variants)

# Model download settings (for Docker Compose)
# Option 1: Use a model from the lex-au HuggingFace collection (default)
ORPHEUS_MODEL_REPO=lex-au # HuggingFace repository owner
# Option 2: Use a custom GGUF from any URL (leave ORPHEUS_MODEL_REPO empty and set ORPHEUS_MODEL_URL)
# ORPHEUS_MODEL_URL=https://example.com/path/to/your/model.gguf
# Examples of custom model URLs:
# ORPHEUS_MODEL_URL=https://huggingface.co/username/repo-name/resolve/main/model.gguf
# ORPHEUS_MODEL_URL=https://your-server.com/models/custom-orpheus.gguf

# Web UI settings (keep in mind that the web UI is not secure and should not be exposed to the internet)
ORPHEUS_PORT=5005
ORPHEUS_HOST=0.0.0.0
